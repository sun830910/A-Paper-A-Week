## A Neural Probabilistic Language Model

### 目的

因测试样本与训练样本存在单词训练上的差异而导致维度灾难(one -hot表示)上的问题，故本文通过提出学习各个单词的分布式表示来解决维度问题。

### 贡献

将神经网络引入语言模型的训练中，并得到了词嵌入这个副产物。  

词嵌入对后面深度学习在自然语言处理方面有较大的贡献，也是获得词的语义特征的有效方法。

### 小结

统计语言模型的目的在于学习单词序列中各个词之间的联合概率分布函数，模型通过训练集可以对指数级的样本进行建模，同时学习**每个单词的分布式表示**以及**单词序列的概率函数**。

传统方法中使用N-gram模型可以得到不错的效果，但会面临维度过高的问题。

在语言模型中的条件函数表达的是在前一个单词为某个单词X的情况下，下一个单词为某个单词Y的概率。这个函数的参数可以在迭代后贴合训练数据的最大对数似然。